\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
\geometry{left=3cm, right=2cm, top=3cm, bottom=2cm}
\usepackage[portuguese,brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{calc}
\usepackage{listings}
\usepackage{float}
\usepgfplotslibrary{statistics}

\usepackage{xcolor} % necessário para cores

\definecolor{codeback}{RGB}{245,245,248}
\definecolor{string}{RGB}{0,120,0}
\definecolor{keyword}{RGB}{0,0,180}
\definecolor{comment}{RGB}{120,120,120}
\definecolor{number}{RGB}{180,0,0}

\lstset{
    backgroundcolor=\color{codeback},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{keyword}\bfseries,
    stringstyle=\color{string},
    commentstyle=\color{comment}\itshape,
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=10pt,
    stepnumber=1,
    showspaces=false,
    showstringspaces=false,
    tabsize=4,
    frame=single,
    frameround=tttt,
    rulecolor=\color{black!30},
    breaklines=true,
    breakatwhitespace=true,
    captionpos=b,
    abovecaptionskip=10pt,
    belowcaptionskip=10pt,
    xleftmargin=15pt,
    xrightmargin=10pt,
    columns=flexible,
    keepspaces=true,
    escapeinside={(*@}{@*)} % permite LaTeX dentro do código
}

% Estilo específico para Python
\lstdefinestyle{python}{
    language=Python,
    morekeywords={*,import,as,from,None,True,False,self,np},
    emph={array,arange,sum,mean,std,max,min,dot,cross,sin,exp,log},
    emphstyle=\color{blue}\bfseries
}

% Estilo específico para C#
\lstdefinestyle{csharp}{
    language=[Sharp]C,
    morekeywords={using,var,new,public,private,static,void,double,float,int,bool},
    emph={Array,List,Enumerable,Range,Select,Sum,Average},
    emphstyle=\color{blue}\bfseries
}

\pgfplotsset{compat=1.18}
\numberwithin{equation}{section}

\title{Revisão e Notas sobre Estatística I}
\author{Pedro Rupf Pereira Viana}
\date{\today}

\begin{document}

\maketitle
\newpage

\section{Introdução e Objetivos}

Trata-se de uma revisão teórica e expositiva sobre Conceitos gerais, Análise Exploratória de Dados (EDA), Amostragem, Probabilidade, Distribuição Normal e o Teorema 
Central do Limite (TCL), bem como suas aplicações na área de Análise e Ciência de Dados.

\section{Conceitos Gerais}

A palavra \textbf{Estatística} têm sua origem na palavra latina \textit{STATUS} (Estado). Há indícios de que há 3000 anos A.C. já eram realizados censos na Babilônia, 
China e Egito. Também podemos ver no livro de Números, do Velho Testamento, uma instrução do Altíssimo dada à Moisés, para que fosse realizado um levantamento dos 
homens de Israel que estivessem aptos para guerrear (Nm. 1:1-3). Usualmente, estas informações eram utilizadas para a taxação de impostos ou para o alistamento militar.

A estatística envolve técnicas para coletar, organizar, descrever, analisar e interpretar dados, quer sejam provenientes de experimentos, ou vindos de estudos 
observacionais. A análise estatística de dados geralmente tem por objetivo a tomada de decisões, resoluções de problemas ou produção de conhecimento. Mas novos 
conhecimentos normalmente geram problemas de pesquisas, resultando em um processo iterativo.

Normalmente o termo estatística esta associado a números, tabelas, gráficos, mas a importância da estatística fica melhor representada por dois pontos comuns em nosso 
dia a dia:
\begin{itemize}
    \item Dados
    \item Variabilidade
\end{itemize}
Podemos dividir a estatística em três áreas: 
\begin{itemize}
    \item Estatística Descritiva
    \item Probabilística
    \item Estatística Inferencial
\end{itemize}
A estatística descritiva é a etapa inicial da análise utilizada para descrever e resumir os dados. A disponibilidade de uma grande quantidade de dados e de métodos 
computacionais muito eficientes revigorou está área da estatística.

Inferência Estatística é, em suma, o estudo de técnicas fundamentada na teoria das probabilidades, que possibilitam a extrapolação a um grande conjunto de dados, das 
informações e conclusões obtidas a partir da amostra.

\subsection{Estatística Descritiva}

A estatística descritiva é um ramo fundamental da estatística que se concentra em resumir, organizar e apresentar dados de forma clara e concisa, sem fazer 
inferências sobre populações maiores. Ela permite que pesquisadores, analistas e tomadores de decisão compreendam as características principais de um conjunto de 
dados, facilitando a identificação de padrões, tendências e anomalias. De acordo com definições padrão, a estatística descritiva utiliza coeficientes breves para 
sintetizar um conjunto de dados, representando uma população inteira ou uma amostra dela. Essa abordagem é essencial em diversas áreas, como economia, saúde, 
educação e ciências sociais, onde o volume de dados pode ser esmagador sem ferramentas de síntese.

Historicamente, a estatística descritiva evoluiu a partir de métodos simples de contagem e agrupamento, ganhando sofisticação com o avanço da computação. Hoje, ela é 
distinguida da estatística inferencial, que usa amostras para fazer previsões sobre populações. Em resumo, enquanto a inferencial projeta para o desconhecido, a 
descritiva descreve o conhecido. Para ilustrar, imagine um conjunto de dados sobre as alturas de alunos em uma turma: a descritiva calcularia médias e variações, mas 
não generalizaria para todos os alunos do país.

\subsubsection{Tipos de dados e variáveis em Estatística Descritiva}

Antes de mergulhar nas medidas, é crucial entender os tipos de dados manipulados na estatística descritiva. Os dados podem ser classificados como qualitativos 
(categóricos) ou quantitativos (numéricos). Os qualitativos incluem categorias nominais (sem ordem, como cores) e ordinais (com ordem, como níveis de satisfação: 
baixo, médio, alto). Já os quantitativos dividem-se em discretos (valores inteiros, como número de filhos) e contínuos (valores reais, como peso).

Essa classificação influencia as técnicas descritivas aplicadas. Por exemplo, para dados qualitativos, usamos frequências e percentuais, enquanto para quantitativos, 
aplicamos medidas numéricas. Além disso, as variáveis podem ser independentes (causadoras) ou dependentes (afetadas), embora na descritiva o foco seja mais na 
descrição do que na causalidade. Uma análise descritiva eficaz começa com a identificação correta desses tipos, evitando erros na interpretação.

\subsubsection{Medidas de Tendência Central}

As medidas de tendência central são o cerne da estatística descritiva, resumindo um conjunto de dados em um único valor representativo. As principais são a 
\textbf{média}, a \textbf{mediana} e a \textbf{moda}.
\begin{itemize}
    \item \textbf{Média Aritmética:} Calculada como a soma de todos os valores, divididos pelo número de observações:
    \begin{eqnarray}
	    \bar{x} = \frac{\sum x_i}{n}\
    \end{eqnarray}
    Ela é sensível a valores extremos (chamados de \textit{outliers}), o que a torna útil para dados simétricos, mas menos robusta em distribuições assimétricas. Por 
    exemplo, em um conjunto de salários {1000, 1200, 1300, 50000}, a média é alta devido ao outlier.
    \item \textbf{Mediana:} O valor central quando os dados são ordenados. Para um número ímpar de observações, é o meio; para par, a média dos dois centrais. Ela é 
    resistente a outliers, ideal para dados enviesados, como rendas familiares.
    \item \textbf{Moda:} O valor mais frequente. Pode ser unimodal (uma moda), bimodal (duas) ou multimodal. Útil para dados categóricos, como a cor mais comum em uma 
    pesquisa.
\end{itemize}
Essas medidas fornecem insights sobre o "centro" dos dados, mas devem ser usadas em conjunto para uma visão completa. Em aplicações reais, como análise de desempenho 
escolar, a mediana pode revelar uma tendência mais realista do que a média influenciada por notas extremas.

\subsubsection{Medidas de Dispersão}

Enquanto as medidas de tendência central indicam onde os dados se concentram, as de dispersão mostram o quão espalhados eles estão. Elas quantificam a variabilidade, 
essencial para entender a consistência dos dados.
\begin{itemize}
    \item \textbf{Amplitude:} A diferença entre o valor máximo e mínimo (Amplitude = Máx - Mín). Simples, mas sensível a outliers.
    \item \textbf{Variância:} A média dos quadrados das diferenças em relação à média:
    \begin{eqnarray}
	    s^2 = \frac{\sum (x_i - \bar{x})^2}{n-1}\
    \end{eqnarray}
    Mede a dispersão quadrática, útil em análises financeiras para risco.
    \item \textbf{Desvio Padrão:} A raiz quadrada da variância $(s = \sqrt{s^2})$. Na mesma unidade dos dados, facilita interpretações, como em controle de 
    qualidade industrial.
    \item \textbf{Intervalo Interquartil (IQR):} A diferença entre o terceiro quartil (Q3) e o primeiro (Q1), capturando a dispersão do meio 50\% dos dados. Robusto 
    contra outliers, usado em boxplots.
\end{itemize}
Essas medidas ajudam a comparar conjuntos de dados. Por exemplo, duas turmas com a mesma média de notas podem ter desvios padrões diferentes, indicando variabilidades 
distintas.

\subsubsection{Distribuições de Frequência e Representações Gráficas}

A estatística descritiva não se limita a números; ela emprega gráficos para visualização. As distribuições de frequência agrupam dados em classes, mostrando contagens ou percentuais. Por exemplo, uma tabela de frequência para idades pode revelar picos em faixas etárias.

Gráficos comuns incluem:
\begin{itemize}
    \item \textbf{Histograma:} Barras representando frequências em intervalos, ideal para dados contínuos, revelando formas como normal (sino) ou assimétrica.
    \vspace{8pt}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Histograma das notas de 30 alunos},
        xlabel={Nota (0--100)},
        ylabel={Frequência absoluta},
        ybar interval,
        xtick={0,20,40,60,80,100},
        ymin=0,
        width=9.5cm,
        height=6cm,
        grid=major
    ]
    \addplot +[hist={bins=10, data min=35, data max=98}]
        table [y index=0] {
        45 52 58 61 63 65 67 68 70 72
        73 75 76 78 79 80 82 84 85 87
        88 89 90 91 92 94 95 96 97 98
    };
    \end{axis}
    \end{tikzpicture}
    \par\vspace{8pt}
    \text{Histograma com 10 classes}

    \item \textbf{Gráfico de Barras:} Para dados categóricos, comparando frequências.
    \vspace{8pt}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Preferência de sabor de sorvete (n=200)},
        ybar,
        bar width=18pt,
        ylabel={Número de pessoas},
        symbolic x coords={Chocolate, Baunilha, Morango, Limão, Coco},
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
        ymin=0,
        ymax=70,
        width=9.5cm,
        height=6cm
    ]
    \addplot coordinates {
        (Chocolate,62) (Baunilha,55) (Morango,48) (Limão,20) (Coco,15)
    };
    \end{axis}
    \end{tikzpicture}
    \par\vspace{8pt}
    \text{Gráfico de barras categóricas}

    \item \textbf{Boxplot:} Mostra mediana, quartis e outliers, facilitando detecção de anomalias.
    \vspace{8pt}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Boxplot das notas por turma},
        ylabel={Nota},
        height=6.5cm,
        width=9cm,
        ytick={0,20,40,60,80,100},
        xtick=\empty
    ]
    \addplot+ [boxplot prepared={
        median=78, upper quartile=85, lower quartile=65,
        upper whisker=98, lower whisker=42},
    ] coordinates {};
    \addplot+ [boxplot prepared={
        median=72, upper quartile=80, lower quartile=60,
        upper whisker=92, lower whisker=48},
    ] coordinates {};
    \addplot+ [boxplot prepared={
        median=82, upper quartile=90, lower quartile=70,
        upper whisker=100, lower whisker=55},
    ] coordinates {};
    \legend{Turma A, Turma B, Turma C}
    \end{axis}
    \end{tikzpicture}
    \par\vspace{8pt}
    \text{Boxplot comparativo de três turmas}

    \item \textbf{Gráfico de Dispersão:} Para relações entre duas variáveis quantitativas, indicando correlações.
    \vspace{8pt}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        title={Relação entre horas de estudo e nota final},
        xlabel={Horas de estudo por semana},
        ylabel={Nota final (0--100)},
        xmin=0, xmax=45,
        ymin=30, ymax=100,
        grid=major,
        width=9.5cm,
        height=6cm,
        legend pos=north west
    ]
    \addplot[only marks, mark=*, blue] coordinates {
        (5,42) (8,48) (10,55) (12,52) (15,60)
        (18,65) (20,68) (22,72) (25,78) (28,80)
        (30,82) (32,85) (35,88) (38,92) (40,95)
    };
    \addlegendentry{Dados observados}
    \addplot[thick, red, domain=0:45] {32 + 1.52*x};
    \addlegendentry{Regressão linear}
    \end{axis}
    \end{tikzpicture}
    \par\vspace{8pt}
    \text{Gráfico de dispersão com linha de tendência}
\end{itemize}
Essas ferramentas visuais resumem dados complexos, tornando-os acessíveis. Em pesquisas sociais, um histograma pode destacar desigualdades em distribuições de renda.

\subsubsection{Assimetria e Curtose}

Para uma descrição completa, avaliamos a forma da distribuição:
\begin{itemize}
    \item \textbf{Assimetria (Skewness):} Mede o desvio da simetria. Positiva (cauda direita longa), negativa (cauda esquerda) ou zero (simétrica).
    \begin{eqnarray}
        Skew = \frac{3(\bar{x} - Mediana)}{s}
    \end{eqnarray}
    \item \textbf{Curtose:} Indica o achatamento das caudas. Alta curtose significa caudas pesadas (mais outliers); baixa, caudas leves.
\end{itemize}
Essas medidas ajudam a escolher testes estatísticos apropriados e detectar desvios de normalidade.

\section{Análise Exploratória de Dados}

\subsection{Conceito básico}

A Análise Exploratória de Dados (Exploratory Data Analysis, ou EDA, na sigla em inglês) representa um pilar fundamental na ciência de dados e na estatística aplicada, 
servindo como etapa inicial e indispensável para compreender conjuntos de dados complexos antes de proceder a modelagens mais avançadas. A EDA enfatiza a importância 
de investigar os dados de forma aberta e iterativa, sem pressuposições rígidas, para revelar padrões, anomalias e relacionamentos subjacentes. Diferentemente da 
análise confirmatória, que testa hipóteses pré-definidas, a EDA adota uma perspectiva indutiva, guiada pela curiosidade empírica e pela robustez estatística.

Deve se entender que a EDA não é apenas como uma coleção de técnicas descritivas, mas como um \textit{framework} metodológico ancorado em princípios probabilísticos 
e inferenciais. Em um contexto onde os dados crescem exponencialmente (impulsionados por avanços em \textit{Big Data} e \textit{Machine Learning}), a EDA assume um 
papel crucial na mitigação de vieses, na detecção de erros e na formulação de hipóteses informadas.

\subsection{Fundamentos Teóricos da Análise Exploratória de Dados}

A EDA baseia-se em premissas estatísticas que priorizam a resiliência dos dados a suposições paramétricas. Em via de regra, não necessariamente os dados irão aderir a 
distribuições ideais, como a normal, e que métodos resistentes (isto é, aqueles menos sensíveis a outliers ou violações de normalidade) devem ser privilegiados. 
Por exemplo, em vez de depender exclusivamente da média aritmética, que é vulnerável a valores extremos, a EDA promove o uso da mediana como medida de tendência 
central mais robusta.

Do ponto de vista probabilístico, a EDA incorpora conceitos de distribuições empíricas e funções de distribuição cumulativa (CDF). A CDF empírica, definida como:
\begin{eqnarray}
    \widehat{F}_{(x)} = \frac{1}{n}\sum_{i = 1}^{n} I(X_{i} \leqslant x)
\end{eqnarray}
onde $I$ é a função indicadora e $n$ o tamanho da amostra, permite uma representação não paramétrica da distribuição dos dados, facilitando comparações com 
distribuições teóricas via testes como o de Kolmogorov-Smirnov. Essa abordagem estatística permite identificar desvios sistemáticos, como assimetria (skewness) ou 
curtose (kurtosis), quantificados respectivamente por:
\begin{eqnarray}
    \gamma_1 = \frac{\mu_3}{\sigma^3} \text{ e } \gamma_2 = \frac{\mu_4}{\sigma^4} - 3
\end{eqnarray}
onde $\mu_k$ são os momentos centrais e $\sigma$ o desvio padrão, respectivamente.

Além disso, a EDA integra princípios de inferência bayesiana informal, onde priors subjetivos são atualizados com evidências dos dados. Embora não formalize modelos 
bayesianos completos, a iteração exploratória refina crenças iniciais sobre a estrutura dos dados, alinhando-se à filosofia de Tukey de "detecção de surpresas". 
Em resumo, os fundamentos teóricos da EDA com viés estatístico enfatizam a robustez, a não parametricidade e a iteração, preparando o terreno para análises mais 
rigorosas.

\subsection{Técnicas Estatísticas Centrais na EDA}

As técnicas estatísticas formam o cerne da EDA, divididas em univariadas, bivariadas e multivariadas. Na análise univariada, o foco recai sobre a distribuição 
individual de variáveis. Medidas de tendência central — média ($\bar{x} = \frac{1}{n} \sum x_i$), mediana e moda — são complementadas por medidas de dispersão, como 
variância ($s^2 = \frac{1}{n-1} \sum (x_i - \bar{x})^2$), desvio padrão e intervalo interquartil (IQR = Q3 - Q1). O IQR, em particular, é estatisticamente robusto 
para detectar outliers via regra de Tukey: valores abaixo de Q1 - 1.5 × IQR ou acima de Q3 + 1.5 × IQR são considerados anômalos.

A análise de distribuições envolve testes de normalidade, como o Shapiro-Wilk, que testa a hipótese nula de que os dados seguem uma distribuição normal: 
\begin{eqnarray}
    W = \frac{(\sum a_i x_{(i)})^2}{\sum (x_i - \bar{x})^2}
\end{eqnarray}
onde $a_i$ são coeficientes tabulados. Se rejeitada, distribuições alternativas (e.g., log-normal, Poisson) 
são exploradas. Para dados categóricos, frequências relativas e testes qui-quadrados revelam associações inesperadas:
\begin{eqnarray}
    \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\end{eqnarray}

Na análise bivariada, coeficientes de correlação são pivôs. O coeficiente de Pearson:
\begin{eqnarray}c
    r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{eqnarray}
assume linearidade e normalidade, enquanto o de Spearman, baseado em ranks, é não paramétrico e robusto a outliers. Para variáveis categóricas, o coeficiente de 
contingência ou o teste de independência qui-quadrado é aplicado. Análises multivariadas estendem isso via matrizes de correlação e análise de componentes principais 
(PCA), onde autovalores e autovetores decompõem a variância: $\mathbf{X} = \mathbf{T} \mathbf{P}^T + \mathbf{E}$, reduzindo dimensionalidade e identificando clusters 
latentes.

\subsection{Ferramentas Visuais e Computacionais com Ênfase Estatística}

A visualização é o braço direito da EDA, transformando abstrações estatísticas em insights intuitivos. Histogramas e gráficos de densidade kernel (KDE) ilustram 
distribuições, com o KDE calculado pela expressão abaixo:
\begin{eqnarray}
    \hat{f}(x) = \frac{1}{nh} \sum K\left(\frac{x - x_i}{h}\right)
\end{eqnarray}
onde $K$ é o kernel e $h$ o bandwidth. Boxplots, introduzidos por Tukey, resumem quartis e outliers, facilitando comparações entre grupos via testes não paramétricos.

Scatterplots e heatmaps de correlação visualizam relacionamentos bivariados, com linhas de regressão linear:
\begin{eqnarray}
    \hat{y} = b_0 + b_1 x, \text{ onde } b_1 = r \frac{s_y}{s_x}
\end{eqnarray}
destacando tendências. Para dados multivariados, pairplots e gráficos de coordenadas paralelas revelam interações complexas. Em contextos estatísticos avançados, 
Q-Q plots comparam quantis empíricos com teóricos, diagnosticando desvios de normalidade.

Computacionalmente, linguagens como \textbf{R} (com pacotes como \textit{ggplot2} e \textit{exploratory}) e Python (com \textit{pandas}, \textit{seaborn} e 
\textit{statsmodels}) operacionalizam essas técnicas. Por exemplo, em Python, a função \textbf{pandas.describe()} gera sumários estatísticos, enquanto 
\textbf{scipy.stats} executa testes. A integração com machine learning estende a EDA para descoberta de padrões não supervisionados, mantendo o viés estatístico via 
validação cruzada e métricas como o coeficiente de silhueta.

\newpage

\section{Amostragem}

\subsection{Conceito básico}

A amostragem constitui um dos pilares fundamentais da estatística inferencial, permitindo que conclusões sobre uma população inteira sejam extraídas a partir do 
estudo de uma subparte dela, denominada amostra. Em contextos onde o censo — a análise exaustiva de todos os elementos da população — é impraticável devido a 
limitações de tempo, custo ou recursos, a amostragem emerge como uma ferramenta indispensável. Conceitualmente, a população refere-se ao conjunto completo de 
indivíduos ou unidades de interesse, caracterizada por parâmetros como média $\mu$ e variância $\sigma^2$, enquanto a amostra é um subconjunto selecionado, cujas 
estatísticas (e.g., média amostral $\bar{x}$ e variância $s^2$) servem como estimadores desses parâmetros.

A teoria da amostragem enfatiza a importância de métodos que minimizem vieses e erros amostrais, garantindo a representatividade e a validade inferencial. 
Distinguem-se dois grandes paradigmas: a amostragem probabilística, onde cada elemento da população tem probabilidade conhecida e não nula de ser selecionado, 
permitindo o cálculo de erros padrão e intervalos de confiança; e a amostragem não probabilística, baseada em critérios subjetivos, mais suscetível a vieses, mas útil 
em pesquisas exploratórias ou com populações de difícil acesso.

\subsection{Fundamentação Teórica da Amostragem}

A amostragem probabilística baseia-se no princípio de que a seleção aleatória reduz o viés de seleção, permitindo que a distribuição amostral de uma estatística siga 
leis probabilísticas conhecidas, como o Teorema Central do Limite (TCL). Pelo TCL, para amostras suficientemente grandes ($n > 30$), a média amostral $\bar{x}$ 
aproxima-se de uma distribuição normal com média $\mu$ e variância $\sigma^2 / n$, independentemente da distribuição populacional.

O erro amostral, quantificado pelo erro padrão, é dado por $SE = \sigma / \sqrt{n}$ para a média, com correção para populações finitas:
\begin{eqnarray}
    SE = \frac{\sigma}{\sqrt{n}} \sqrt{\frac{N-n}{N-1}}
\end{eqnarray}
onde $N$ é o tamanho populacional. Já o viés ocorre quando o método de seleção sistematicamente super ou subestima o parâmetro. Em amostragens não probabilísticas, 
o viés é incontrolável, impedindo generalizações rigorosas. Outro conceito chave é o quadro amostral (sampling frame), lista completa e atualizada da população, 
essencial para evitar erros de cobertura.

\subsection{Métodos de Amostragem Probabilística}

Os métodos probabilísticos garantem inferência estatística válida. São eles:
\begin{itemize}
    \item \textbf{Amostragem Aleatória Simples (AAS):} Cada elemento tem probabilidade igual de seleção ($p = n/N$). Pode ser com ou sem reposição. Fórmula de 
    variância da média:
    \begin{eqnarray}
        Var(\bar{x}) = \frac{\sigma^2}{n} \left(1 - \frac{n}{N}\right)
    \end{eqnarray}
    Simples e sem viés, mas ineficiente para populações heterogêneas.
    \item \textbf{Amostragem Sistemática:} Seleciona-se um elemento inicial aleatório $k$ e, em seguida, cada $i$-ésimo ($i = N/n$). Eficiente operacionalmente, mas 
    suscetível a periodicidades na lista.
    \item \textbf{Amostragem Estratificada:} A população é dividida em estratos homogêneos (e.g., por idade ou renda), com seleção aleatória proporcional ou ótima 
    dentro de cada. Reduz a expressão da variância por:
    \begin{eqnarray}
        Var(\bar{x}_{est}) = \sum W_h^2 \frac{\sigma_h^2}{n_h}
    \end{eqnarray}
    onde $W_h$ é o peso do estrato. Ideal para heterogeneidade conhecida.
    \item \textbf{Amostragem por Conglomerados (Cluster):} Divide-se em clusters heterogêneos (e.g., escolas), selecionando clusters aleatoriamente e amostrando 
    todos ou subamostra dentro. Útil para populações dispersas, mas aumenta variância se clusters forem semelhantes internamente.
\end{itemize}

\subsection{Métodos de Amostragem Não Probabilística}

Embora menos rigorosos, são comuns em pesquisas qualitativas:
\begin{itemize}
    \item \textbf{Amostragem por Conveniência:} Seleção de elementos acessíveis. Rápida, mas propensa a viés.
    \item \textbf{Amostragem por Cotas:} Similar à estratificada, mas sem aleatoriedade; fixa cotas por características.
    \item \textbf{Amostragem Intencional ou por Julgamento:} É onde o especialista seleciona elementos típicos.
    \item \textbf{AAmostragem "Bola de Neve":} Indicada para populações raras; participantes indicam outros.
\end{itemize}
Esses métodos não permitem cálculo de erros padrão, limitando a generalização.

\subsection{Dimensionamento Amostral}

O tamanho da amostra $n$ é determinado por fórmulas que equilibram precisão e custo. Para estimar uma média com margem de erro $e$ e nível de confiança 
$1-\alpha$ (tipicamente 95\%, $z = 1.96$):
\begin{eqnarray}
    n = \frac{z^2 \sigma^2}{e^2}
\end{eqnarray}
Para populações finitas: 
\begin{eqnarray}
    n = \frac{n_0}{1 + \frac{n_0 - 1}{N}}
\end{eqnarray}
onde $n_0$ é o $n$ infinito.

Para proporções ($p$ estimada, frequentemente 0.5 para máxima variância):
\begin{eqnarray}
    n = \frac{z^2 p(1-p)}{e^2}
\end{eqnarray}
Fatores como poder estatístico (em testes de hipóteses) e efeito esperado também influenciam.

\section{Probabilidade}

\subsection{Conceito Básico}

A teoria da probabilidade constitui o alicerce matemático para o estudo de fenômenos aleatórios, fornecendo ferramentas rigorosas para quantificar incerteza e modelar 
eventos incertos. Surgida no século XVII com contribuições de Blaise Pascal e Pierre de Fermat no contexto de jogos de azar, e formalizada por Andrey Kolmogorov em 
1933 por meio de uma abordagem axiomática, a probabilidade transcende sua origem lúdica para se tornar indispensável em campos como estatística inferencial, física 
quântica, inteligência artificial e economia.

Do ponto de vista matemático, a probabilidade é uma medida sobre espaços de eventos, satisfazendo axiomas que garantem consistência lógica. Em estatística, ela 
sustenta a inferência, permitindo que amostras informem sobre populações via distribuições probabilísticas. Este texto explora os fundamentos axiomáticos, conceitos 
chave como probabilidade condicional e independência, distribuições de probabilidade, teoremas limitantes centrais e aplicações, enfatizando o rigor matemático e as 
implicações para a análise de dados.

\subsection{Fundamentos e Definições da Probabilidade}

Na literatura podemos encontrar três definições de probabilidade: \textbf{Clássica}, \textbf{Geométrica} e \textbf{Frequentista}.

Na definição clássica todos os elementos de um espaço amostral possuem a mesma chance de acontecerem. Seja um evento \textit{A} de interesse, associado a um espaço 
amostral $\Omega$. Então a probabilidade de ocorrência do evento \textit{A}, será a razão entre o número de elementos do evento de interesse com o número de elementos 
do espaço amostral.
\begin{eqnarray}
    P(A) = \frac{n(A)}{n(\Omega)}
\end{eqnarray}

Considerando que o espaço amostral pode ser \textit{não enumerável}, então o conceito de probabilidades se aplicará ao comprimento de intervalos, medida de áreas ou 
similares, dando origem a probabilidade geométrica.
\begin{eqnarray}
    P(A) = \frac{\text{comprimento de }A}{\text{comprimento de }\Omega}
\end{eqnarray}

Por fim, na definição frequêntista, deve-se considerar o limite das frequências relativas como o valor da probabilidade. Assim, seja $n_{A}$ o número de ocorrências 
de \textit{A} em \textit{n} repetições independentes do experimento em questão, temos:
\begin{eqnarray}
    P(A) = \lim_{n \to \infty} \frac{n_{A}}{n}
\end{eqnarray}

Em síntese, isso significa que a medida que são realizados os experimentos, a probabilidade de ocorrência de um evento determinado se aproxima do verdadeiro valor a
medida que o número de realizações tende ao infinito.

Para isso, considere um experimento que consiste em lançar uma moeda 10, 50, 100 e 1000 vezes, e observar o número de ensaios em que o resultado é cara. Os resultados 
podem ser verificados na Figura 01 abaixo. Observe que a medida que o número de ensaios cresce a probabilidade acumulada da ocorrência de cara converge para sua 
verdadeira ocorrência, isto é, $P(A) = 0,5$.
\begin{figure}[H]
		\centering
		\includegraphics[width=15cm]{003.png}
		\caption{Convergência da frequência relativa de caras ao lançar uma moeda várias vezes.}
\end{figure}

Considerando que as definições anteriores são úteis para o cálculo de diversos problemas práticos e teóricos, é necessário enunciar uma série de axiomas para que se
tenha uma formulação mais rigorosa para o conceito de probabilidade. Por volta do ano de 1930 o russo A. N. Kolmogorov apresentou esses axiomas matemáticos para 
definir probabilidade.
\begin{itemize}
    \item \textbf{Axioma 1: }Para qualquer evento \textit{A}, $P(A) \geqslant 0$.
    
    O \textbf{Axioma 1} de Kolmogorov é assumido como verdadeiro por definição. Não há prova derivada; é um postulado fundamental que garante que probabilidades não 
    sejam negativas.
    \item \textbf{Axioma 2: }Seja $\Omega$ o espaço amostral associado ao experimento aleatório, então $P(\Omega) = 1$.
    
    O \textbf{Axioma 2} de Kolmogorov também é assumido por definição, pois representa a certeza de que algo no espaço amostral ocorre.
    \item \textbf{Axioma 3: }Se $A_{1},A_{2},\cdots,A_{k}$ for um conjunto finito de eventos mutuamente exclusivos, isto é, $A_{i} \cap A_{j} = \varnothing$, então
    $P(\bigcup_{i = 1}^{k} A_{i}) = \sum_{i = 1}^{k} P(A_{i})$.

    O \textbf{Axioma 3} de Kolmogorov também é assumido por definição, pois para coleções finitas, reduz-se a $P(E_1 \cup \cdots \cup E_n) = P(E_1) + \cdots + P(E_n)$ 
    quando disjuntos.
\end{itemize}

Ou seja, No experimento aleatório de lançar uma moeda e observar a face superior, o espaço amostral é $\Omega = {H,T}$. Pelos axiomas citados anteriormente, tem-se
que $P(\Omega) = 1$. Logo:
\begin{eqnarray}
    1 = P(\Omega) = P(H \cup T) = P(H) + P(T) = 0,5 + 0,5
\end{eqnarray}

\subsection{Propriedades Derivadas}

As propriedades abaixo são teoremas provados a partir dos axiomas supracitados.

\begin{enumerate}
    \item \textbf{Probabilidade do conjunto vazio}:
    \[
    P(\emptyset) = 0.
    \]
    \textit{Demonstração}: Note que $\emptyset \cup \emptyset = \emptyset$. Pelo Axioma 3,
    \[
    P(\emptyset) = P(\emptyset \cup \emptyset) = P(\emptyset) + P(\emptyset) \implies P(\emptyset) = 0.
    \]

    \item \textbf{Probabilidade do complemento}: Para qualquer evento $A$,
    \[
    P(A^c) = 1 - P(A).
    \]
    \textit{Demonstração}: Temos $A \cup A^c = \Omega$ e $A \cap A^c = \emptyset$. Pelo Axioma 3 e Axioma 2,
    \[
    P(\Omega) = P(A \cup A^c) = P(A) + P(A^c) = 1 \implies P(A^c) = 1 - P(A).
    \]

    \item \textbf{Monotonicidade}: Se $A \subseteq B$, então
    \[
    P(A) \leq P(B).
    \]
    \textit{Demonstração}: Se $A \subseteq B$, então $B = A \cup (B \setminus A)$ com $A \cap (B \setminus A) = \emptyset$. Pelo Axioma 3,
    \[
    P(B) = P(A) + P(B \setminus A).
    \]
    Como $P(B \setminus A) \geq 0$ (Axioma 1), segue que $P(B) \geq P(A)$.

    \item \textbf{Limites da probabilidade}: Para qualquer evento $A$,
    \[
    0 \leq P(A) \leq 1.
    \]
    \textit{Demonstração}: 
    \begin{itemize}
        \item $P(A) \geq 0$ vem do Axioma 1.
        \item Como $A \subseteq \Omega$, pela monotonicidade $P(A) \leq P(\Omega) = 1$. Ou ainda: $P(A) + P(A^c) = 1$ e $P(A^c) \geq 0 \implies P(A) \leq 1$.
    \end{itemize}

    \item \textbf{Princípio de inclusão-exclusão para dois eventos}:
    \[
    P(A \cup B) = P(A) + P(B) - P(A \cap B).
    \]
    \textit{Demonstração}: Escreva $A \cup B = A \cup (B \setminus A)$, onde $B \setminus A = B \cap A^c$ e os conjuntos são disjuntos. Pelo Axioma 3,
    \[
    P(A \cup B) = P(A) + P(B \setminus A).
    \]
    Agora, $B = (A \cap B) \cup (B \setminus A)$ com interseção vazia, logo
    \[
    P(B) = P(A \cap B) + P(B \setminus A) \implies P(B \setminus A) = P(B) - P(A \cap B).
    \]
    Substituindo,
    \[
    P(A \cup B) = P(A) + P(B) - P(A \cap B).
    \]

    \item \textbf{Probabilidade da união de eventos disjuntos (caso finito)}: Se $A \cap B = \emptyset$,
    \[
    P(A \cup B) = P(A) + P(B).
    \]
    \textit{Demonstração}: Caso particular do Axioma 3 ou do princípio de inclusão-exclusão:
    \[
    P(A \cup B) = P(A) + P(B) - P(\emptyset) = P(A) + P(B).
    \]
\end{enumerate}

Essas são as propriedades mais fundamentais e comumente usadas na probabilidade clássica. Elas valem tanto para espaços de probabilidade finitos quanto contínuos, 
desde que os axiomas de Kolmogorov sejam satisfeitos.

\section{Distribuição Normal}

A distribuição normal, frequentemente chamada de curva de sino ou distribuição gaussiana, é uma das ferramentas mais fundamentais e versáteis da estatística e da 
probabilidade. Ela descreve como os valores de uma variável se distribuem ao redor de um ponto central, com a maioria dos dados concentrados no meio e diminuindo 
simetricamente para os lados. Imagine uma montanha simétrica, onde o pico representa o valor mais comum, e as encostas mostram como os valores se tornam menos 
frequentes à medida que se afastam do centro. Essa forma é tão comum na natureza e nos dados humanos que a distribuição normal se tornou um pilar para entender 
padrões em tudo, desde alturas de pessoas até erros de medição em experimentos científicos.

\subsection{Origem}

A distribuição normal tem raízes no século XVIII, quando matemáticos como Abraham de Moivre começaram a estudá-la como uma aproximação para distribuições binomiais em 
jogos de azar. Por exemplo, ao lançar uma moeda muitas vezes, a probabilidade de obter um número de caras próximo à metade do total se concentra em torno de um valor 
médio, formando algo semelhante a uma curva de sino. No início do século XIX, Pierre-Simon Laplace e Carl Friedrich Gauss refinaram esses ideias, aplicando-as a erros 
de observação em astronomia. Gauss, em particular, usou a distribuição para modelar variações em medições celestes, o que levou à sua associação com o nome 
"gaussiana".

Ao longo do século XIX, figuras como Adolphe Quetelet aplicaram o conceito a dados sociais, como alturas e pesos de populações humanas, demonstrando que muitos traços 
biológicos seguem padrões normais. Francis Galton, um pioneiro na estatística, admirava tanto essa distribuição que a descreveu como uma "lei de frequência de erro" 
que revelava uma ordem cósmica. No século XX, com o avanço da estatística inferencial por Ronald Fisher e outros, a normal se consolidou como base para testes 
estatísticos e modelagens. Hoje, em uma era de big data e inteligência artificial, ela continua relevante, adaptada a contextos digitais e computacionais.

Essa evolução reflete uma transição de curiosidades matemáticas para uma ferramenta essencial na ciência empírica. A distribuição normal não foi "inventada", mas 
descoberta como um padrão recorrente, destacando como a matemática pode capturar a essência da variabilidade no mundo.

\subsection{Definição Formal e Função Densidade}

Seja $X$ uma variável aleatória contínua. Dizemos que $X$ segue uma \textbf{distribuição normal} com média $\mu$ e variância $\sigma^2 > 0$ (denotada 
$X \sim \mathcal{N}(\mu,\sigma^2)$) quando sua função densidade de probabilidade é:
\begin{eqnarray}
    f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right], \qquad -\infty < x < +\infty
\end{eqnarray}
onde:
\begin{itemize}
    \item $\mu$ (média) $\to$ centro da curva (pico do sino)
    \item $\sigma$ (desvio padrão) $\to$ controla a largura do sino
    \item $\sigma^2$ (variância) $\to$ medida de dispersão ao quadrado
\end{itemize}

\subsection{Distribuição Normal Padrão}

A normal padrão é o caso particular em que $\mu = 0$ e $\sigma = 1$. Denotamos $Z \sim \mathcal{N}(0,1)$. Sua densidade é:
\begin{eqnarray}
    \phi(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}\right)
\end{eqnarray}

Qualquer normal pode ser padronizada pela transformação:
\begin{eqnarray}
    Z = \frac{X - \mu}{\sigma}
\end{eqnarray}

\subsection{Principais Propriedades}

\begin{itemize}
    \item Simetria perfeita: $f(\mu + t) = f(\mu - t)$ para todo $t$
    \item Média $=$ Mediana $=$ Moda $=$ $\mu$
    \item Variância $=$ $\sigma^2$ e desvio padrão $=$ $\sigma$
    \item Assimetria (skewness) $=$ 0
    \item Curtose excessiva $=$ 0 (curva mesocúrtica)
    \item Função geradora de momentos:
    \[
    M(t) = \exp\left(\mu t + \frac{\sigma^2 t^2}{2}\right)
    \]
    \item \textbf{Reprodutividade (fechamento sob combinações lineares):}\\
    Se $X_i \sim \mathcal{N}(\mu_i,\sigma_i^2)$ forem independentes e $a_i, b$ constantes, então
    \[
    \sum_{i} a_i X_i + b \;\sim\; \mathcal{N}\left(\sum_{i} a_i \mu_i + b,\; \sum_{i} a_i^2 \sigma_i^2\right)
    \]
\end{itemize}

\subsection{Regra Empírica (68–95–99,7)}

\begin{align*}
P(|\,X - \mu\,| < \sigma)      &\approx 0{,}683 \\
P(|\,X - \mu\,| < 2\sigma)     &\approx 0{,}954 \\
P(|\,X - \mu\,| < 3\sigma)     &\approx 0{,}997
\end{align*}

Na normal padrão:

\begin{align*}
P(-1 < Z < 1) &\approx 68{,}3\% \\
P(-2 < Z < 2) &\approx 95{,}4\% \\
P(-3 < Z < 3) &\approx 99{,}7\%
\end{align*}

\subsection{Função de Distribuição Cumulativa}

\[
\Phi(z) = P(Z \leq z) = \int_{-\infty}^{z} \phi(t)\,dt
\]

Não possui forma fechada, mas está extensivamente tabelada e implementada em softwares.

\section{Teorema Central do Limite (versão clássica)}

A ubiquidade da distribuição normal não é acidental; ela é explicada por teoremas que mostram como ela surge como limite de muitos processos aleatórios. O Teorema 
Central do Limite é o mais famoso: quando você soma muitas variáveis aleatórias independentes, independentemente de suas distribuições originais (desde que tenham 
variabilidade finita), a soma tende a se comportar como uma normal à medida que o número de variáveis aumenta. Por exemplo, a média de muitos lançamentos de dados, 
mesmo que cada dado seja uniforme, aproxima-se de uma curva de sino.

Isso explica por que fenômenos compostos por múltiplos fatores independentes — como o crescimento de plantas influenciado por genes, solo e clima — frequentemente 
seguem padrões normais. Em estatística, esse teorema justifica o uso da normal para aproximar distribuições amostrais, permitindo inferências sobre populações a 
partir de amostras. Sem ele, muitas análises estatísticas modernas seriam impraticáveis.

Outros resultados semelhantes, como aproximações para distribuições binomiais em grandes amostras, reforçam essa centralidade. Em resumo, a normal não é imposta; ela 
emerge organicamente da soma de influências aleatórias, refletindo a complexidade do mundo real.

\subsection{Definição}

Sejam $X_1,\dots,X_n$ variáveis aleatórias i.i.d. com média $\mu$ e variância $\sigma^2 < \infty$. Então, quando $n\to\infty$,

\[
\sqrt{n}\;\frac{\bar{X}_n - \mu}{\sigma} \;\xrightarrow{d}\; \mathcal{N}(0,1)
\]

Essa é a razão profunda pela qual a normal aparece “em todo lugar”.

\subsection{Exemplos Práticos de Uso das Equações}

\begin{itemize}
    \item \textbf{Intervalo de confiança de 95\% para a média} ($\sigma$ conhecida):
    \[
    \bar{X} \pm 1{,}96 \cdot \frac{\sigma}{\sqrt{n}}
    \]
    \item \textbf{Estatística do teste z para média}:
    \[
    z = \frac{\bar{X} - \mu_0}{\sigma / \sqrt{n}}
    \]
    \item \textbf{Regressão linear clássica} assume:
    \[
    \varepsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0,\sigma^2)
    \]
\end{itemize}

\end{document}